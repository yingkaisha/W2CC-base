{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe482de-6ad1-438b-9df5-306064461747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a57168-76cd-431e-aa6a-f280212e959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zarr\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5cecda-1d2c-493c-aa03-957afe95fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53879a61-42cf-4a43-8c28-46cef84c2a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "887732a0-ad4b-48ec-8d45-4e43d95e225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8fee044-34d4-4347-a2e5-2d9f033dde5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_lead_and_stack(ds_list, new_dim=\"member\", labels=None, ref=\"first\"):\n",
    "    \"\"\"\n",
    "    ds_list: list[xr.Dataset], each has a 'time' coord with daily values\n",
    "    new_dim: name of the new dimension to stack on\n",
    "    labels: optional labels for new_dim (len == len(ds_list))\n",
    "    ref: \"first\" (per-dataset first time) or a numpy/pandas datetime-like scalar\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i, ds in enumerate(ds_list):\n",
    "        ds = ds.sortby(\"time\")\n",
    "\n",
    "        if ref == \"first\":\n",
    "            t0 = ds[\"time\"].isel(time=0)\n",
    "        else:\n",
    "            # global reference (same for all ds)\n",
    "            t0 = xr.DataArray(ref)\n",
    "\n",
    "        lead = (ds[\"time\"] - t0).astype(\"timedelta64[D]\")  # daily deltas\n",
    "        ds2 = ds.assign_coords(lead_time=(\"time\", lead.data)).swap_dims({\"time\": \"lead_time\"})\n",
    "        ds2 = ds2.drop_vars(\"time\")  # optional: remove original coordinate\n",
    "\n",
    "        out.append(ds2)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = list(range(len(out)))\n",
    "\n",
    "    stacked = xr.concat(out, dim=xr.IndexVariable(new_dim, labels))\n",
    "    return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd54f27d-e5a1-41c6-ad17-8dfb62cafc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detrend_linear(da, dim=\"time\"):\n",
    "    \"\"\"\n",
    "    Remove a best-fit linear trend along `dim` for each grid point.\n",
    "    Uses an index-based time axis (0..N-1) to avoid datetime scaling issues.\n",
    "    \"\"\"\n",
    "    t = xr.DataArray(np.arange(da.sizes[dim]), dims=dim, coords={dim: da[dim]})\n",
    "\n",
    "    valid = np.isfinite(da)\n",
    "    t_valid = t.where(valid)\n",
    "    da_valid = da.where(valid)\n",
    "\n",
    "    t_mean = t_valid.mean(dim, skipna=True)\n",
    "    y_mean = da_valid.mean(dim, skipna=True)\n",
    "\n",
    "    cov = ((t_valid - t_mean) * (da_valid - y_mean)).mean(dim, skipna=True)\n",
    "    var = ((t_valid - t_mean) ** 2).mean(dim, skipna=True)\n",
    "\n",
    "    slope = cov / var\n",
    "    intercept = y_mean - slope * t_mean\n",
    "\n",
    "    trend = slope * t + intercept\n",
    "    return da - trend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246b0bb-697f-4a2d-9307-8b1a111a44c4",
   "metadata": {},
   "source": [
    "## CESM metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bd21a0a-8966-42ea-87f6-f966e4cd38b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Pituffik/CESM_metrics.zarr\n",
      "Elapsed: 43.043062 s\n",
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Fairbanks/CESM_metrics.zarr\n",
      "Elapsed: 41.059032 s\n",
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Guam/CESM_metrics.zarr\n",
      "Elapsed: 40.405697 s\n",
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Yuma_PG/CESM_metrics.zarr\n",
      "Elapsed: 41.526583 s\n",
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Fort_Bragg/CESM_metrics.zarr\n",
      "Elapsed: 41.141597 s\n"
     ]
    }
   ],
   "source": [
    "station_names = ['Pituffik', 'Fairbanks', 'Guam', 'Yuma_PG' ,'Fort_Bragg'] # \n",
    "\n",
    "for stn in station_names:\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    \n",
    "    base_dir = f'/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/{stn}/'\n",
    "    \n",
    "    # ========================== #\n",
    "    # get data\n",
    "    list_ds = []\n",
    "    for year in range(1958, 2020):\n",
    "        fn = f'/glade/campaign/ral/hap/ksha/EPRI_data/CESM_SMYLE_daily/{stn}_{year}.zarr'\n",
    "        ds = xr.open_zarr(fn)\n",
    "        list_ds.append(ds)\n",
    "    ds_all = time_to_lead_and_stack(list_ds, new_dim=\"init_time\")\n",
    "    ds_all = ds_all.assign_coords({'init_time': np.arange(1959, 2021)})\n",
    "    lead_year = (ds_all[\"lead_time\"] / np.timedelta64(365, \"D\")).astype(int)\n",
    "    ds_all = ds_all.assign_coords(lead_year=(\"lead_time\", lead_year.data))\n",
    "    \n",
    "    # ========================== #\n",
    "    # get anomaly\n",
    "    ds_all_anom = ds_all.copy()\n",
    "    vars_ = list(ds_all.keys())\n",
    "    for v in vars_:\n",
    "        ds_all_anom[v] = ds_all_anom[v] - ds_all_anom[v].mean([\"init_time\"])\n",
    "    ds_all_anom = ds_all_anom[vars_]\n",
    "    \n",
    "    # ========================== #\n",
    "    # get detrend data\n",
    "    ds_all_detrend = ds_all.copy()\n",
    "    vars_ = list(ds_all.keys())\n",
    "    for v in vars_:\n",
    "        ds_all_detrend[v] = detrend_linear(ds_all[v], dim=\"init_time\")\n",
    "    ds_all_detrend = ds_all_detrend[vars_]\n",
    "    \n",
    "    # ======================= #\n",
    "    # metrics\n",
    "    ds_group = ds_all.groupby(\"lead_year\")\n",
    "    ds_max  = ds_group.max(dim=\"lead_time\",  skipna=True)\n",
    "    ds_min  = ds_group.min(dim=\"lead_time\",  skipna=True)\n",
    "    ds_mean  = ds_group.mean(dim=\"lead_time\",  skipna=True)\n",
    "    ds_30d = ds_group.map(\n",
    "        lambda x: x.rolling(lead_time=30, min_periods=30).mean().max(dim=\"lead_time\", skipna=True)\n",
    "    )\n",
    "    ds_min = ds_min.rename({'TREFHTMN': 'TREFHTMN_min'})[['TREFHTMN_min',]]\n",
    "    ds_max = ds_max.rename({'PRECT': 'PRECT_max', 'TREFHTMX': 'TREFHTMX_max'})[['PRECT_max', 'TREFHTMX_max']]\n",
    "    ds_30d = ds_30d.rename({'TREFHTMX': 'TREFHTMX_30d', 'PRECT': 'PRECT_30d'})[['TREFHTMX_30d', 'PRECT_30d']]\n",
    "    ds_mean = ds_mean.rename({'PRECT': 'PRECT_mean', 'TREFHT': 'TREFHT_mean'})[['PRECT_mean', 'TREFHT_mean']]\n",
    "    ds_metrics = xr.merge([ds_min, ds_max, ds_30d, ds_mean])\n",
    "    ds_metrics = ds_metrics.rename({v: f\"{v}_default\" for v in ds_metrics.data_vars})\n",
    "    \n",
    "    # ========================== #\n",
    "    # anomaly metrics\n",
    "    ds_group = ds_all_anom.groupby(\"lead_year\")\n",
    "    ds_max  = ds_group.max(dim=\"lead_time\",  skipna=True)\n",
    "    ds_min  = ds_group.min(dim=\"lead_time\",  skipna=True)\n",
    "    ds_mean  = ds_group.mean(dim=\"lead_time\",  skipna=True)\n",
    "    ds_30d = ds_group.map(\n",
    "        lambda x: x.rolling(lead_time=30, min_periods=30).mean().max(dim=\"lead_time\", skipna=True)\n",
    "    )\n",
    "    ds_min = ds_min.rename({'TREFHTMN': 'TREFHTMN_min'})[['TREFHTMN_min',]]\n",
    "    ds_max = ds_max.rename({'PRECT': 'PRECT_max', 'TREFHTMX': 'TREFHTMX_max'})[['PRECT_max', 'TREFHTMX_max']]\n",
    "    ds_30d = ds_30d.rename({'TREFHTMX': 'TREFHTMX_30d', 'PRECT': 'PRECT_30d'})[['TREFHTMX_30d', 'PRECT_30d']]\n",
    "    ds_mean = ds_mean.rename({'PRECT': 'PRECT_mean', 'TREFHT': 'TREFHT_mean'})[['PRECT_mean', 'TREFHT_mean']]\n",
    "    ds_metrics_anom = xr.merge([ds_min, ds_max, ds_30d, ds_mean])\n",
    "    ds_metrics_anom = ds_metrics_anom.rename({v: f\"{v}_anom\" for v in ds_metrics_anom.data_vars})\n",
    "    \n",
    "    # ========================== #\n",
    "    # detrended metrics\n",
    "    ds_group = ds_all_detrend.groupby(\"lead_year\")\n",
    "    ds_max  = ds_group.max(dim=\"lead_time\",  skipna=True)\n",
    "    ds_min  = ds_group.min(dim=\"lead_time\",  skipna=True)\n",
    "    ds_mean  = ds_group.mean(dim=\"lead_time\",  skipna=True)\n",
    "    ds_30d = ds_group.map(\n",
    "        lambda x: x.rolling(lead_time=30, min_periods=30).mean().max(dim=\"lead_time\", skipna=True)\n",
    "    )\n",
    "    ds_min = ds_min.rename({'TREFHTMN': 'TREFHTMN_min'})[['TREFHTMN_min',]]\n",
    "    ds_max = ds_max.rename({'PRECT': 'PRECT_max', 'TREFHTMX': 'TREFHTMX_max'})[['PRECT_max', 'TREFHTMX_max']]\n",
    "    ds_30d = ds_30d.rename({'TREFHTMX': 'TREFHTMX_30d', 'PRECT': 'PRECT_30d'})[['TREFHTMX_30d', 'PRECT_30d']]\n",
    "    ds_mean = ds_mean.rename({'PRECT': 'PRECT_mean', 'TREFHT': 'TREFHT_mean'})[['PRECT_mean', 'TREFHT_mean']]\n",
    "    ds_metrics_detrend = xr.merge([ds_min, ds_max, ds_30d, ds_mean])\n",
    "    ds_metrics_detrend = ds_metrics_detrend.rename({v: f\"{v}_detrend\" for v in ds_metrics_detrend.data_vars})\n",
    "    \n",
    "    # ========================== #\n",
    "    # save\n",
    "    ds_final = xr.merge([ds_metrics, ds_metrics_anom, ds_metrics_detrend])\n",
    "    save_name = base_dir + 'CESM_metrics.zarr'\n",
    "    ds_final.to_zarr(save_name, mode='w')\n",
    "    print(save_name)\n",
    "    \n",
    "    t1 = time.perf_counter()\n",
    "    print(f\"Elapsed: {t1 - t0:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b3b413-0dcf-4757-826f-20887108b089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615568ec-7bab-442e-aa8e-5b6c6cc4b7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0fd00-5581-4f5a-a9ff-ae2e56c392dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0fa513-9c72-482f-af9d-6d379639cb21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623dd14-5830-4908-8680-518a5676c86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
