{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe482de-6ad1-438b-9df5-306064461747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a57168-76cd-431e-aa6a-f280212e959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zarr\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5cecda-1d2c-493c-aa03-957afe95fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53879a61-42cf-4a43-8c28-46cef84c2a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "887732a0-ad4b-48ec-8d45-4e43d95e225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8fee044-34d4-4347-a2e5-2d9f033dde5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_lead_and_stack(ds_list, new_dim=\"member\", labels=None, ref=\"first\"):\n",
    "    \"\"\"\n",
    "    ds_list: list[xr.Dataset], each has a 'time' coord with daily values\n",
    "    new_dim: name of the new dimension to stack on\n",
    "    labels: optional labels for new_dim (len == len(ds_list))\n",
    "    ref: \"first\" (per-dataset first time) or a numpy/pandas datetime-like scalar\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i, ds in enumerate(ds_list):\n",
    "        ds = ds.sortby(\"time\")\n",
    "\n",
    "        if ref == \"first\":\n",
    "            t0 = ds[\"time\"].isel(time=0)\n",
    "        else:\n",
    "            # global reference (same for all ds)\n",
    "            t0 = xr.DataArray(ref)\n",
    "\n",
    "        lead = (ds[\"time\"] - t0).astype(\"timedelta64[D]\")  # daily deltas\n",
    "        ds2 = ds.assign_coords(lead_time=(\"time\", lead.data)).swap_dims({\"time\": \"lead_time\"})\n",
    "        ds2 = ds2.drop_vars(\"time\")  # optional: remove original coordinate\n",
    "\n",
    "        out.append(ds2)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = list(range(len(out)))\n",
    "\n",
    "    stacked = xr.concat(out, dim=xr.IndexVariable(new_dim, labels))\n",
    "    return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd54f27d-e5a1-41c6-ad17-8dfb62cafc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detrend_linear(da, dim=\"time\"):\n",
    "    \"\"\"\n",
    "    Remove a best-fit linear trend along `dim` for each grid point.\n",
    "    Uses an index-based time axis (0..N-1) to avoid datetime scaling issues.\n",
    "    \"\"\"\n",
    "    t = xr.DataArray(np.arange(da.sizes[dim]), dims=dim, coords={dim: da[dim]})\n",
    "\n",
    "    valid = np.isfinite(da)\n",
    "    t_valid = t.where(valid)\n",
    "    da_valid = da.where(valid)\n",
    "\n",
    "    t_mean = t_valid.mean(dim, skipna=True)\n",
    "    y_mean = da_valid.mean(dim, skipna=True)\n",
    "\n",
    "    cov = ((t_valid - t_mean) * (da_valid - y_mean)).mean(dim, skipna=True)\n",
    "    var = ((t_valid - t_mean) ** 2).mean(dim, skipna=True)\n",
    "\n",
    "    slope = cov / var\n",
    "    intercept = y_mean - slope * t_mean\n",
    "\n",
    "    trend = slope * t + intercept\n",
    "    return da - trend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246b0bb-697f-4a2d-9307-8b1a111a44c4",
   "metadata": {},
   "source": [
    "## CESM metrics (detrend per predicted year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed2c05b5-423f-4bb4-9d66-f4ede02acf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# station_names = ['Pituffik', 'Fairbanks', 'Guam', 'Yuma_PG' ,'Fort_Bragg'] # \n",
    "\n",
    "# for stn in station_names:\n",
    "    \n",
    "#     t0 = time.perf_counter()\n",
    "    \n",
    "#     base_dir = f'/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/{stn}/'\n",
    "    \n",
    "#     # ========================== #\n",
    "#     # get data\n",
    "#     list_ds = []\n",
    "#     for year in range(1958, 2020):\n",
    "#         fn = f'/glade/campaign/ral/hap/ksha/EPRI_data/CESM_SMYLE_STN/{stn}_{year}.zarr'\n",
    "#         ds = xr.open_zarr(fn)[['PRECT', 'TREFHT', 'TREFHTMX', 'TREFHTMN']]\n",
    "#         list_ds.append(ds)\n",
    "        \n",
    "#     ds_all = time_to_lead_and_stack(list_ds, new_dim=\"init_time\")\n",
    "#     ds_all['PRECT'] = ds_all['PRECT'] * 60*60*24 * 1000 # mm per day\n",
    "#     ds_all = ds_all.chunk({\"init_time\": -1})\n",
    "    \n",
    "#     # ========================== #\n",
    "#     ds_all = ds_all.assign_coords({'init_time': np.arange(1959, 2021)})\n",
    "#     lead_year = (ds_all[\"lead_time\"] / np.timedelta64(365, \"D\")).astype(int)\n",
    "#     ds_all = ds_all.assign_coords(lead_year=(\"lead_time\", lead_year.data))\n",
    "    \n",
    "#     # ========================== #\n",
    "#     # get anomaly  (lead-year-dependent mean removed; same mean for all days in a lead_year)\n",
    "#     ds_all_anom = ds_all.copy()\n",
    "#     vars_ = list(ds_all.keys())\n",
    "#     for v in vars_:\n",
    "#         clim = ds_all[v].groupby(\"lead_year\").mean(\n",
    "#             dim=(\"init_time\", \"lead_time\"), skipna=True\n",
    "#         )  # -> dims: lead_year\n",
    "#         ds_all_anom[v] = ds_all[v].groupby(\"lead_year\") - clim\n",
    "#     ds_all_anom = ds_all_anom[vars_]\n",
    "    \n",
    "#     # ========================== #\n",
    "#     # get detrend data (lead-year-dependent linear trend vs init_time; same trend for all days in a lead_year)\n",
    "#     ds_all_detrend = ds_all.copy()\n",
    "#     vars_ = list(ds_all.keys())\n",
    "    \n",
    "#     t = ds_all[\"init_time\"]\n",
    "#     t0 = t.mean(\"init_time\")\n",
    "    \n",
    "#     for v in vars_:\n",
    "#         # 1) collapse each lead_year to an \"annual mean\" per init_time\n",
    "#         da_yr = ds_all[v].groupby(\"lead_year\").mean(dim=\"lead_time\", skipna=True)\n",
    "#         # da_yr dims: (init_time, lead_year, ...)\n",
    "    \n",
    "#         # 2) fit linear trend across init_time for each lead_year\n",
    "#         coef = da_yr.polyfit(dim=\"init_time\", deg=1, skipna=True)[\"polyfit_coefficients\"]\n",
    "#         slope = coef.sel(degree=1)  # dims: (lead_year, ...)\n",
    "    \n",
    "#         # 3) remove ONLY the trend term (keeps mean level)\n",
    "#         trend_term = slope * (t - t0)  # dims: (lead_year, init_time, ...)\n",
    "#         ds_all_detrend[v] = ds_all[v].groupby(\"lead_year\") - trend_term\n",
    "    \n",
    "#     ds_all_detrend = ds_all_detrend[vars_]\n",
    "    \n",
    "#     # ======================= #\n",
    "#     # metrics\n",
    "#     ds_group = ds_all.groupby(\"lead_year\")\n",
    "#     ds_max  = ds_group.max(dim=\"lead_time\",  skipna=True)\n",
    "#     ds_min  = ds_group.min(dim=\"lead_time\",  skipna=True)\n",
    "#     ds_mean  = ds_group.mean(dim=\"lead_time\",  skipna=True)\n",
    "#     ds_30d = ds_group.map(\n",
    "#         lambda x: x.rolling(lead_time=30, min_periods=30).mean().max(dim=\"lead_time\", skipna=True)\n",
    "#     )\n",
    "#     ds_min = ds_min.rename({'TREFHTMN': 'TREFHTMN_min', 'TREFHT': 'TREFHT_min'})[['TREFHTMN_min', 'TREFHT_min']]\n",
    "#     ds_max = ds_max.rename({'PRECT': 'PRECT_max', 'TREFHTMX': 'TREFHTMX_max', 'TREFHT': 'TREFHT_max'})[['PRECT_max', 'TREFHTMX_max', 'TREFHT_max']]\n",
    "#     ds_30d = ds_30d.rename({'TREFHT': 'TREFHT_30d', 'PRECT': 'PRECT_30d'})[['TREFHT_30d', 'PRECT_30d']]\n",
    "#     ds_mean = ds_mean.rename({'PRECT': 'PRECT_mean', 'TREFHT': 'TREFHT_mean'})[['PRECT_mean', 'TREFHT_mean']]\n",
    "#     ds_metrics = xr.merge([ds_min, ds_max, ds_30d, ds_mean])\n",
    "#     ds_metrics = ds_metrics.rename({v: f\"{v}_default\" for v in ds_metrics.data_vars})\n",
    "    \n",
    "#     # ========================== #\n",
    "#     # anomaly metrics\n",
    "#     ds_group = ds_all_anom.groupby(\"lead_year\")\n",
    "#     ds_max  = ds_group.max(dim=\"lead_time\",  skipna=True)\n",
    "#     ds_min  = ds_group.min(dim=\"lead_time\",  skipna=True)\n",
    "#     ds_mean  = ds_group.mean(dim=\"lead_time\",  skipna=True)\n",
    "#     ds_30d = ds_group.map(\n",
    "#         lambda x: x.rolling(lead_time=30, min_periods=30).mean().max(dim=\"lead_time\", skipna=True)\n",
    "#     )\n",
    "#     ds_min = ds_min.rename({'TREFHTMN': 'TREFHTMN_min', 'TREFHT': 'TREFHT_min'})[['TREFHTMN_min', 'TREFHT_min']]\n",
    "#     ds_max = ds_max.rename({'PRECT': 'PRECT_max', 'TREFHTMX': 'TREFHTMX_max', 'TREFHT': 'TREFHT_max'})[['PRECT_max', 'TREFHTMX_max', 'TREFHT_max']]\n",
    "#     ds_30d = ds_30d.rename({'TREFHT': 'TREFHT_30d', 'PRECT': 'PRECT_30d'})[['TREFHT_30d', 'PRECT_30d']]\n",
    "#     ds_mean = ds_mean.rename({'PRECT': 'PRECT_mean', 'TREFHT': 'TREFHT_mean'})[['PRECT_mean', 'TREFHT_mean']]\n",
    "#     ds_metrics_anom = xr.merge([ds_min, ds_max, ds_30d, ds_mean])\n",
    "#     ds_metrics_anom = ds_metrics_anom.rename({v: f\"{v}_anom\" for v in ds_metrics_anom.data_vars})\n",
    "    \n",
    "#     # ========================== #\n",
    "#     # detrended metrics\n",
    "#     ds_group = ds_all_detrend.groupby(\"lead_year\")\n",
    "#     ds_max  = ds_group.max(dim=\"lead_time\",  skipna=True)\n",
    "#     ds_min  = ds_group.min(dim=\"lead_time\",  skipna=True)\n",
    "#     ds_mean  = ds_group.mean(dim=\"lead_time\",  skipna=True)\n",
    "#     ds_30d = ds_group.map(\n",
    "#         lambda x: x.rolling(lead_time=30, min_periods=30).mean().max(dim=\"lead_time\", skipna=True)\n",
    "#     )\n",
    "#     ds_min = ds_min.rename({'TREFHTMN': 'TREFHTMN_min', 'TREFHT': 'TREFHT_min'})[['TREFHTMN_min', 'TREFHT_min']]\n",
    "#     ds_max = ds_max.rename({'PRECT': 'PRECT_max', 'TREFHTMX': 'TREFHTMX_max', 'TREFHT': 'TREFHT_max'})[['PRECT_max', 'TREFHTMX_max', 'TREFHT_max']]\n",
    "#     ds_30d = ds_30d.rename({'TREFHT': 'TREFHT_30d', 'PRECT': 'PRECT_30d'})[['TREFHT_30d', 'PRECT_30d']]\n",
    "#     ds_mean = ds_mean.rename({'PRECT': 'PRECT_mean', 'TREFHT': 'TREFHT_mean'})[['PRECT_mean', 'TREFHT_mean']]\n",
    "#     ds_metrics_detrend = xr.merge([ds_min, ds_max, ds_30d, ds_mean])\n",
    "#     ds_metrics_detrend = ds_metrics_detrend.rename({v: f\"{v}_detrend\" for v in ds_metrics_detrend.data_vars})\n",
    "    \n",
    "#     # ========================== #\n",
    "#     # save\n",
    "#     ds_final = xr.merge([ds_metrics, ds_metrics_anom, ds_metrics_detrend])\n",
    "#     save_name = base_dir + 'CESM_metrics.zarr'\n",
    "#     ds_final = ds_final.chunk({'lead_year': 10, 'init_time': 62})\n",
    "#     ds_final.to_zarr(save_name, mode='w')\n",
    "#     print(save_name)\n",
    "    \n",
    "#     t1 = time.perf_counter()\n",
    "#     print(f\"Elapsed: {t1 - t0:.6f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5522d-0050-49aa-8f2e-811d87a69a9e",
   "metadata": {},
   "source": [
    "### No detrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bd21a0a-8966-42ea-87f6-f966e4cd38b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Pituffik/CESM_metrics.zarr\n",
      "Elapsed: 34.945242 s\n",
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Fairbanks/CESM_metrics.zarr\n",
      "Elapsed: 21.880670 s\n",
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Guam/CESM_metrics.zarr\n",
      "Elapsed: 22.526847 s\n",
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Yuma_PG/CESM_metrics.zarr\n",
      "Elapsed: 22.020775 s\n",
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Fort_Bragg/CESM_metrics.zarr\n",
      "Elapsed: 22.612251 s\n"
     ]
    }
   ],
   "source": [
    "station_names = ['Pituffik', 'Fairbanks', 'Guam', 'Yuma_PG' ,'Fort_Bragg'] # \n",
    "\n",
    "for stn in station_names:\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    \n",
    "    base_dir = f'/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/{stn}/'\n",
    "    \n",
    "    # ========================== #\n",
    "    # get data\n",
    "    list_ds = []\n",
    "    for year in range(1958, 2020):\n",
    "        fn = f'/glade/campaign/ral/hap/ksha/EPRI_data/CESM_SMYLE_STN/{stn}_{year}.zarr'\n",
    "        ds = xr.open_zarr(fn)[['PRECT', 'TREFHT', 'TREFHTMX', 'TREFHTMN']]\n",
    "        list_ds.append(ds)\n",
    "        \n",
    "    ds_all = time_to_lead_and_stack(list_ds, new_dim=\"init_time\")\n",
    "    \n",
    "    ds_all['PRECT'] = ds_all['PRECT'] * 60*60*24 * 1000 # mm per day\n",
    "    \n",
    "    ds_all = ds_all.assign_coords({'init_time': np.arange(1959, 2021)})\n",
    "    lead_year = (ds_all[\"lead_time\"] / np.timedelta64(365, \"D\")).astype(int)\n",
    "    ds_all = ds_all.assign_coords(lead_year=(\"lead_time\", lead_year.data))\n",
    "    \n",
    "    # ======================= #\n",
    "    # metrics\n",
    "    ds_group = ds_all.groupby(\"lead_year\")\n",
    "    ds_max  = ds_group.max(dim=\"lead_time\",  skipna=True)\n",
    "    ds_min  = ds_group.min(dim=\"lead_time\",  skipna=True)\n",
    "    ds_mean  = ds_group.mean(dim=\"lead_time\",  skipna=True)\n",
    "    ds_30d = ds_group.map(\n",
    "        lambda x: x.rolling(lead_time=30, min_periods=30).mean().max(dim=\"lead_time\", skipna=True)\n",
    "    )\n",
    "    ds_min = ds_min.rename({'TREFHTMN': 'TREFHTMN_min', 'TREFHT': 'TREFHT_min'})[['TREFHTMN_min', 'TREFHT_min']]\n",
    "    ds_max = ds_max.rename({'PRECT': 'PRECT_max', 'TREFHTMX': 'TREFHTMX_max', 'TREFHT': 'TREFHT_max'})[['PRECT_max', 'TREFHTMX_max', 'TREFHT_max']]\n",
    "    ds_30d = ds_30d.rename({'TREFHT': 'TREFHT_30d', 'PRECT': 'PRECT_30d'})[['TREFHT_30d', 'PRECT_30d']]\n",
    "    ds_mean = ds_mean.rename({'PRECT': 'PRECT_mean', 'TREFHT': 'TREFHT_mean'})[['PRECT_mean', 'TREFHT_mean']]\n",
    "    ds_metrics = xr.merge([ds_min, ds_max, ds_30d, ds_mean])\n",
    "    ds_metrics = ds_metrics.rename({v: f\"{v}_default\" for v in ds_metrics.data_vars})\n",
    "    \n",
    "    # ========================== #\n",
    "    # save\n",
    "    ds_final = ds_metrics\n",
    "    #xr.merge([ds_metrics, ds_metrics_anom, ds_metrics_detrend])\n",
    "    save_name = base_dir + 'CESM_metrics.zarr'\n",
    "    ds_final.to_zarr(save_name, mode='w')\n",
    "    print(save_name)\n",
    "    \n",
    "    t1 = time.perf_counter()\n",
    "    print(f\"Elapsed: {t1 - t0:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623dd14-5830-4908-8680-518a5676c86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516e1ff-1af0-4067-8ad6-dd96c23d7302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
