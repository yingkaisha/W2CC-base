{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe482de-6ad1-438b-9df5-306064461747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a57168-76cd-431e-aa6a-f280212e959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zarr\n",
    "import time\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd \n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b374bd3-1087-42bc-8240-f40e5615c2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xclim.indices import (\n",
    "    standardized_precipitation_evapotranspiration_index,\n",
    "    water_budget,\n",
    ")\n",
    "\n",
    "from xclim.indices.stats import (\n",
    "    standardized_index_fit_params, \n",
    "    standardized_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee680705-54a7-4d01-8d8c-e17b59114311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Converting a CFTimeIndex.*noleap.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "887732a0-ad4b-48ec-8d45-4e43d95e225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04b1d2bb-1ecd-410c-aee5-41f6b4ccaf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_lead_and_stack(ds_list, new_dim=\"member\", labels=None, ref=\"first\"):\n",
    "    \"\"\"\n",
    "    ds_list: list[xr.Dataset], each has a 'time' coord with daily values\n",
    "    new_dim: name of the new dimension to stack on\n",
    "    labels: optional labels for new_dim (len == len(ds_list))\n",
    "    ref: \"first\" (per-dataset first time) or a numpy/pandas datetime-like scalar\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i, ds in enumerate(ds_list):\n",
    "        ds = ds.sortby(\"time\")\n",
    "\n",
    "        if ref == \"first\":\n",
    "            t0 = ds[\"time\"].isel(time=0)\n",
    "        else:\n",
    "            # global reference (same for all ds)\n",
    "            t0 = xr.DataArray(ref)\n",
    "\n",
    "        lead = (ds[\"time\"] - t0).astype(\"timedelta64[D]\")  # daily deltas\n",
    "        ds2 = ds.assign_coords(lead_time=(\"time\", lead.data)).swap_dims({\"time\": \"lead_time\"})\n",
    "        ds2 = ds2.drop_vars(\"time\")  # optional: remove original coordinate\n",
    "\n",
    "        out.append(ds2)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = list(range(len(out)))\n",
    "\n",
    "    stacked = xr.concat(out, dim=xr.IndexVariable(new_dim, labels))\n",
    "    return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c8e981-4101-4c82-843f-cdd448637dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vars(data):\n",
    "    \"\"\"\n",
    "    Preprocess data variables for SPEI calculation.\n",
    "    \"\"\"\n",
    "    \n",
    "    data[\"time\"] = (pd.to_datetime(data[\"time\"]).to_numpy().astype(\"datetime64[D]\"))\n",
    "    first_date = data.time.min().dt.strftime(\"%Y-%m-%d\").values.item()\n",
    "    \n",
    "    if not first_date.endswith(\"-01-01\"):\n",
    "        first_year = int(first_date[:4])\n",
    "        data = data.sel(time=slice(str(first_year + 1), None))\n",
    "\n",
    "    precip = data[\"precip\"]\n",
    "    tmin = data[\"tmin\"]\n",
    "    tmax = data[\"tmax\"]\n",
    "    \n",
    "    return precip, tmin, tmax\n",
    "    \n",
    "def calc_spei_and_params(\n",
    "    precip, tmin, tmax, \n",
    "    agg_freq, start_date, end_date, \n",
    "    lat=None, dist=\"fisk\", method=\"ML\"):\n",
    "    \n",
    "    # --- Daily water budget for full period\n",
    "    wb = water_budget(pr=precip, tasmin=tmin, tasmax=tmax, method=\"HG85\", lat=lat)\n",
    "    wb.attrs[\"units\"] = \"kg m-2 s-1\"\n",
    "\n",
    "    # --- Fit params on the calibration period (monthly aggregation + rolling handled inside)\n",
    "    params = standardized_index_fit_params(\n",
    "        wb.sel(time=slice(start_date, end_date)),\n",
    "        freq=\"MS\",           # aggregate daily WB to monthly\n",
    "        window=agg_freq,     # e.g., 12 for SPEI-12\n",
    "        dist=dist,           # \"fisk\" = 3-parameter log-logistic\n",
    "        method=method,       # \"PWM\" (L-moments) is robust; \"ML\" for MLE\n",
    "    )\n",
    "\n",
    "    # --- Apply those params to the full record to get SPEI (ensures consistency)\n",
    "    spei = standardized_index(\n",
    "        wb,\n",
    "        freq=\"MS\",\n",
    "        window=agg_freq,\n",
    "        dist=dist,\n",
    "        method=method,\n",
    "        params=params,       # << use fixed calibration parameters\n",
    "        zero_inflated=False,    # for water balance, not zero-inflated\n",
    "        fitkwargs=None,         # or {}\n",
    "        cal_start=None,         # not needed when params are supplied\n",
    "        cal_end=None,   \n",
    "    )\n",
    "\n",
    "    if np.nanmin(spei.values) < -3:\n",
    "        spei = spei.where(spei >= -3, np.nan).interpolate_na(\"time\")\n",
    "\n",
    "    return params, spei\n",
    "\n",
    "def noleap_to_gregorian_add_leap(ds: xr.Dataset, time_dim: str = \"time\") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Convert cftime.DatetimeNoLeap time coord to pandas DatetimeIndex (Gregorian)\n",
    "    and add Feb 29 for leap years by reindexing to a complete daily index and\n",
    "    linearly filling inserted dates.\n",
    "    \"\"\"\n",
    "    # 1) CFTimeIndex -> pandas DatetimeIndex (drops Feb 29 by definition)\n",
    "    cft = ds.indexes[time_dim]                 # xarray.coding.cftimeindex.CFTimeIndex\n",
    "    pd_idx = cft.to_datetimeindex()           # pandas.DatetimeIndex\n",
    "    ds = ds.assign_coords({time_dim: pd_idx})\n",
    "\n",
    "    # 2) Build full daily Gregorian index (includes Feb 29 when applicable)\n",
    "    full_idx = pd.date_range(pd_idx[0], pd_idx[-1], freq=\"D\")\n",
    "\n",
    "    # 3) Reindex to insert missing days (Feb 29 becomes NaN rows)\n",
    "    ds2 = ds.reindex({time_dim: full_idx})\n",
    "\n",
    "    # 4) Fill inserted NaNs by linear interpolation in time\n",
    "    #    (works for numeric variables; keeps non-numeric as-is)\n",
    "    num_vars = [v for v in ds2.data_vars if ds2[v].dtype.kind in \"fiu\"]\n",
    "    ds2[num_vars] = ds2[num_vars].interpolate_na(time_dim, method=\"linear\")\n",
    "\n",
    "    return ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31f73106-8f35-4544-afe8-cc67206807f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_names = ['Pituffik', 'Fairbanks', 'Guam', 'Yuma_PG' ,'Fort_Bragg'] # \n",
    "SPEI_48 = np.empty((len(station_names), 10, 12*(2020-1959+1)))\n",
    "\n",
    "for i_stn, stn in enumerate(station_names):\n",
    "    \n",
    "    for lead_year in range(0, 10):\n",
    "         \n",
    "        start_date = f\"{1959+lead_year}-01-01T00\"\n",
    "        end_date = f\"{2020+lead_year}-12-31T23\"\n",
    "        \n",
    "        ds_collection = []\n",
    "        \n",
    "        # ========================== #\n",
    "        # get data\n",
    "        list_ds = []\n",
    "        for year_init in range(1958, 2020, 1):\n",
    "            \n",
    "            year_start = year_init + 1 + lead_year\n",
    "            time_start = f'{year_start}-01-01T00'\n",
    "            time_end = f'{year_start}-12-31T00'\n",
    "            \n",
    "            fn = f'/glade/campaign/ral/hap/ksha/EPRI_data/CESM_SMYLE_STN/{stn}_{year_init}.zarr'\n",
    "            ds = xr.open_zarr(fn)[['TREFHTMN', 'TREFHTMX', 'PRECT']]\n",
    "            ds = ds.sel(time=slice(time_start, time_end))\n",
    "            list_ds.append(ds)\n",
    "        \n",
    "        ds_all = xr.concat(list_ds, dim='time')\n",
    "        ds_all = ds_all.load()\n",
    "        \n",
    "        cft = ds_all.indexes['time']\n",
    "        pd_idx = cft.to_datetimeindex()\n",
    "        ds_all = ds_all.assign_coords({'time': pd_idx})\n",
    "        \n",
    "        lat_ref = ds_all['lat'].values\n",
    "        lat_mid = lat_ref # lat_ref[ind_lat]\n",
    "        time_vals = ds_all['time']\n",
    "        \n",
    "        tmin = ds_all['TREFHTMN'].values\n",
    "        tmax = ds_all['TREFHTMX'].values\n",
    "        precip = ds_all['PRECT'].values\n",
    "        \n",
    "        ds = xr.Dataset(\n",
    "            {\n",
    "                \"precip\": ((\"time\",), precip*1e3, {\"units\": \"kg m-2 s-1\"}),\n",
    "                \"tmin\":   ((\"time\",), tmin-273.15, {\"units\": \"degC\"}),\n",
    "                \"tmax\":   ((\"time\",), tmax-273.15, {\"units\": \"degC\"}),\n",
    "            },\n",
    "            coords={\"time\": time_vals, \"lat\": lat_mid}\n",
    "        )\n",
    "        \n",
    "        for v in (\"precip\", \"tmin\", \"tmax\"):\n",
    "            \n",
    "            ds[v] = ds[v].assign_coords(lat=lat_mid)\n",
    "            \n",
    "            ds[v][\"lat\"].attrs = {\n",
    "                \"standard_name\": \"latitude\",\n",
    "                \"units\": \"degrees_north\", \"axis\": \"Y\"\n",
    "            }\n",
    "        \n",
    "        precip, tmin, tmax = process_vars(ds)\n",
    "        # ---------------------------------- #\n",
    "        # 48 month lagged SPEI\n",
    "        params, spei = calc_spei_and_params(\n",
    "            precip, tmin, tmax, \n",
    "            agg_freq=1, \n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            lat=precip[\"lat\"],\n",
    "            dist=\"fisk\", method=\"ML\"\n",
    "        )\n",
    "        \n",
    "        SPEI_48[i_stn, lead_year, :,] = spei.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31aeaf36-fbf8-4233-bf3b-b426dd528bfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 37200 into shape (10,62,12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m m_per_year \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# (10, 744, Nx, Ny) -> (10, 62, 12, Nx, Ny)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m tmp_48 \u001b[38;5;241m=\u001b[39m \u001b[43mSPEI_48\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_lead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_per_year\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# (10, 62, 12) -> (62, 10, 12)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m tmp_48 \u001b[38;5;241m=\u001b[39m tmp_48\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 37200 into shape (10,62,12)"
     ]
    }
   ],
   "source": [
    "n_lead = 10\n",
    "n_init = 62\n",
    "m_per_year = 12\n",
    "\n",
    "# (10, 744, Nx, Ny) -> (10, 62, 12, Nx, Ny)\n",
    "tmp_48 = SPEI_48.reshape(n_lead, n_init, m_per_year)\n",
    "\n",
    "# (10, 62, 12) -> (62, 10, 12)\n",
    "tmp_48 = tmp_48.transpose(1, 0, 2)\n",
    "\n",
    "# (62, 10, 12) -> (62, 120)\n",
    "SPEI_init_48 = tmp_48.reshape(n_init, n_lead * m_per_year)\n",
    "\n",
    "ds_SPEI = xr.Dataset(\n",
    "    data_vars={\n",
    "        \"SPEI\": ((\"init_time\", \"lead_time_month\"), SPEI_init_48)\n",
    "    },\n",
    "    coords={\n",
    "        \"init_time\": np.arange(1958, 2020),\n",
    "        \"lead_time_month\": np.arange(120),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e8ac8-8dd0-420f-b27c-003aee947f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0246b0bb-697f-4a2d-9307-8b1a111a44c4",
   "metadata": {},
   "source": [
    "## CESM metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bd21a0a-8966-42ea-87f6-f966e4cd38b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Pituffik/CESM_metrics.zarr\n",
      "Elapsed: 58.345111 s\n",
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Fairbanks/CESM_metrics.zarr\n",
      "Elapsed: 57.277154 s\n",
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Guam/CESM_metrics.zarr\n",
      "Elapsed: 59.746165 s\n",
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Yuma_PG/CESM_metrics.zarr\n",
      "Elapsed: 59.272855 s\n",
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Fort_Bragg/CESM_metrics.zarr\n",
      "Elapsed: 59.190298 s\n"
     ]
    }
   ],
   "source": [
    "station_names = ['Pituffik', 'Fairbanks', 'Guam', 'Yuma_PG' ,'Fort_Bragg'] # \n",
    "\n",
    "for stn in station_names:\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    \n",
    "    base_dir = f'/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/{stn}/'\n",
    "    \n",
    "    # ========================== #\n",
    "    # get data\n",
    "    list_ds = []\n",
    "    for year in range(1958, 2020):\n",
    "        fn = f'/glade/campaign/ral/hap/ksha/EPRI_data/CESM_SMYLE_STN/{stn}_{year}.zarr'\n",
    "        ds = xr.open_zarr(fn)\n",
    "        list_ds.append(ds)\n",
    "        \n",
    "    ds_all = time_to_lead_and_stack(list_ds, new_dim=\"init_time\")\n",
    "    \n",
    "    ds_all['PRECT'] = ds_all['PRECT'] * 60*60*24 * 1000 # mm per day\n",
    "    \n",
    "    ds_all = ds_all.assign_coords({'init_time': np.arange(1959, 2021)})\n",
    "    lead_year = (ds_all[\"lead_time\"] / np.timedelta64(365, \"D\")).astype(int)\n",
    "    ds_all = ds_all.assign_coords(lead_year=(\"lead_time\", lead_year.data))\n",
    "    \n",
    "    # ========================== #\n",
    "    # get anomaly\n",
    "    ds_all_anom = ds_all.copy()\n",
    "    vars_ = list(ds_all.keys())\n",
    "    for v in vars_:\n",
    "        ds_all_anom[v] = ds_all_anom[v] - ds_all_anom[v].mean([\"init_time\"])\n",
    "    ds_all_anom = ds_all_anom[vars_]\n",
    "    \n",
    "    # ========================== #\n",
    "    # get detrend data\n",
    "    ds_all_detrend = ds_all.copy()\n",
    "    vars_ = list(ds_all.keys())\n",
    "    for v in vars_:\n",
    "        ds_all_detrend[v] = detrend_linear(ds_all[v], dim=\"init_time\")\n",
    "    ds_all_detrend = ds_all_detrend[vars_]\n",
    "    \n",
    "    # ======================= #\n",
    "    # metrics\n",
    "    ds_group = ds_all.groupby(\"lead_year\")\n",
    "    ds_max  = ds_group.max(dim=\"lead_time\",  skipna=True)\n",
    "    ds_min  = ds_group.min(dim=\"lead_time\",  skipna=True)\n",
    "    ds_mean  = ds_group.mean(dim=\"lead_time\",  skipna=True)\n",
    "    ds_30d = ds_group.map(\n",
    "        lambda x: x.rolling(lead_time=30, min_periods=30).mean().max(dim=\"lead_time\", skipna=True)\n",
    "    )\n",
    "    ds_min = ds_min.rename({'TREFHTMN': 'TREFHTMN_min', 'TREFHT': 'TREFHT_min'})[['TREFHTMN_min', 'TREFHT_min']]\n",
    "    ds_max = ds_max.rename({'PRECT': 'PRECT_max', 'TREFHTMX': 'TREFHTMX_max', 'TREFHT': 'TREFHT_max'})[['PRECT_max', 'TREFHTMX_max', 'TREFHT_max']]\n",
    "    ds_30d = ds_30d.rename({'TREFHT': 'TREFHT_30d', 'PRECT': 'PRECT_30d'})[['TREFHT_30d', 'PRECT_30d']]\n",
    "    ds_mean = ds_mean.rename({'PRECT': 'PRECT_mean', 'TREFHT': 'TREFHT_mean'})[['PRECT_mean', 'TREFHT_mean']]\n",
    "    ds_metrics = xr.merge([ds_min, ds_max, ds_30d, ds_mean])\n",
    "    ds_metrics = ds_metrics.rename({v: f\"{v}_default\" for v in ds_metrics.data_vars})\n",
    "    \n",
    "    # ========================== #\n",
    "    # anomaly metrics\n",
    "    ds_group = ds_all_anom.groupby(\"lead_year\")\n",
    "    ds_max  = ds_group.max(dim=\"lead_time\",  skipna=True)\n",
    "    ds_min  = ds_group.min(dim=\"lead_time\",  skipna=True)\n",
    "    ds_mean  = ds_group.mean(dim=\"lead_time\",  skipna=True)\n",
    "    ds_30d = ds_group.map(\n",
    "        lambda x: x.rolling(lead_time=30, min_periods=30).mean().max(dim=\"lead_time\", skipna=True)\n",
    "    )\n",
    "    # ds_min = ds_min.rename({'TREFHTMN': 'TREFHTMN_min'})[['TREFHTMN_min',]]\n",
    "    # ds_max = ds_max.rename({'PRECT': 'PRECT_max', 'TREFHTMX': 'TREFHTMX_max'})[['PRECT_max', 'TREFHTMX_max']]\n",
    "    # ds_30d = ds_30d.rename({'TREFHTMX': 'TREFHTMX_30d', 'PRECT': 'PRECT_30d'})[['TREFHTMX_30d', 'PRECT_30d']]\n",
    "    # ds_mean = ds_mean.rename({'PRECT': 'PRECT_mean', 'TREFHT': 'TREFHT_mean'})[['PRECT_mean', 'TREFHT_mean']]\n",
    "    ds_min = ds_min.rename({'TREFHTMN': 'TREFHTMN_min', 'TREFHT': 'TREFHT_min'})[['TREFHTMN_min', 'TREFHT_min']]\n",
    "    ds_max = ds_max.rename({'PRECT': 'PRECT_max', 'TREFHTMX': 'TREFHTMX_max', 'TREFHT': 'TREFHT_max'})[['PRECT_max', 'TREFHTMX_max', 'TREFHT_max']]\n",
    "    ds_30d = ds_30d.rename({'TREFHT': 'TREFHT_30d', 'PRECT': 'PRECT_30d'})[['TREFHT_30d', 'PRECT_30d']]\n",
    "    ds_mean = ds_mean.rename({'PRECT': 'PRECT_mean', 'TREFHT': 'TREFHT_mean'})[['PRECT_mean', 'TREFHT_mean']]\n",
    "    ds_metrics_anom = xr.merge([ds_min, ds_max, ds_30d, ds_mean])\n",
    "    ds_metrics_anom = ds_metrics_anom.rename({v: f\"{v}_anom\" for v in ds_metrics_anom.data_vars})\n",
    "    \n",
    "    # ========================== #\n",
    "    # detrended metrics\n",
    "    ds_group = ds_all_detrend.groupby(\"lead_year\")\n",
    "    ds_max  = ds_group.max(dim=\"lead_time\",  skipna=True)\n",
    "    ds_min  = ds_group.min(dim=\"lead_time\",  skipna=True)\n",
    "    ds_mean  = ds_group.mean(dim=\"lead_time\",  skipna=True)\n",
    "    ds_30d = ds_group.map(\n",
    "        lambda x: x.rolling(lead_time=30, min_periods=30).mean().max(dim=\"lead_time\", skipna=True)\n",
    "    )\n",
    "    ds_min = ds_min.rename({'TREFHTMN': 'TREFHTMN_min', 'TREFHT': 'TREFHT_min'})[['TREFHTMN_min', 'TREFHT_min']]\n",
    "    ds_max = ds_max.rename({'PRECT': 'PRECT_max', 'TREFHTMX': 'TREFHTMX_max', 'TREFHT': 'TREFHT_max'})[['PRECT_max', 'TREFHTMX_max', 'TREFHT_max']]\n",
    "    ds_30d = ds_30d.rename({'TREFHT': 'TREFHT_30d', 'PRECT': 'PRECT_30d'})[['TREFHT_30d', 'PRECT_30d']]\n",
    "    ds_mean = ds_mean.rename({'PRECT': 'PRECT_mean', 'TREFHT': 'TREFHT_mean'})[['PRECT_mean', 'TREFHT_mean']]\n",
    "    ds_metrics_detrend = xr.merge([ds_min, ds_max, ds_30d, ds_mean])\n",
    "    ds_metrics_detrend = ds_metrics_detrend.rename({v: f\"{v}_detrend\" for v in ds_metrics_detrend.data_vars})\n",
    "    \n",
    "    # ========================== #\n",
    "    # save\n",
    "    ds_final = xr.merge([ds_metrics, ds_metrics_anom, ds_metrics_detrend])\n",
    "    save_name = base_dir + 'CESM_metrics.zarr'\n",
    "    ds_final.to_zarr(save_name, mode='w')\n",
    "    print(save_name)\n",
    "    \n",
    "    t1 = time.perf_counter()\n",
    "    print(f\"Elapsed: {t1 - t0:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dece8f-53e4-468f-9367-d74dc024e1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c369fbd-8aee-4618-a27e-59e49c8a0f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54a67e-db5b-43af-914d-321a0c0cc3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0fa513-9c72-482f-af9d-6d379639cb21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623dd14-5830-4908-8680-518a5676c86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
