{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44c9a09-afbd-4e79-841a-ab8f3eccd9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4be19b2-5327-4858-9af6-f0a1b26ebdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc255b0-c91b-428a-9159-eb74eaca8ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xclim.indices import (\n",
    "    standardized_precipitation_evapotranspiration_index,\n",
    "    water_budget,\n",
    ")\n",
    "\n",
    "from xclim.indices.stats import (\n",
    "    standardized_index_fit_params, \n",
    "    standardized_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdbab42f-452e-47dc-90a5-98259033ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Converting a CFTimeIndex.*noleap.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff6332b6-9e01-4686-a6d0-dc4513aa5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "from scipy.spatial import Delaunay\n",
    "from scipy.interpolate import LinearNDInterpolator, NearestNDInterpolator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0500cac4-8b5f-4f33-a6fc-ebd0729c9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03fa39b5-30a6-4472-bdd2-cf5f0199f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detrend_linear(da, dim=\"time\"):\n",
    "    \"\"\"\n",
    "    Remove a best-fit linear trend along `dim` for each grid point.\n",
    "    Uses an index-based time axis (0..N-1) to avoid datetime scaling issues.\n",
    "    \"\"\"\n",
    "    t = xr.DataArray(np.arange(da.sizes[dim]), dims=dim, coords={dim: da[dim]})\n",
    "\n",
    "    valid = np.isfinite(da)\n",
    "    t_valid = t.where(valid)\n",
    "    da_valid = da.where(valid)\n",
    "\n",
    "    t_mean = t_valid.mean(dim, skipna=True)\n",
    "    y_mean = da_valid.mean(dim, skipna=True)\n",
    "\n",
    "    cov = ((t_valid - t_mean) * (da_valid - y_mean)).mean(dim, skipna=True)\n",
    "    var = ((t_valid - t_mean) ** 2).mean(dim, skipna=True)\n",
    "\n",
    "    slope = cov / var\n",
    "    intercept = y_mean - slope * t_mean\n",
    "\n",
    "    trend = slope * t + intercept\n",
    "    return da - trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c480c7a-a275-4f63-89bc-85469fab6c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vars(data):\n",
    "    \"\"\"\n",
    "    Preprocess data variables for SPEI calculation.\n",
    "    \"\"\"\n",
    "    \n",
    "    data[\"time\"] = (pd.to_datetime(data[\"time\"]).to_numpy().astype(\"datetime64[D]\"))\n",
    "    first_date = data.time.min().dt.strftime(\"%Y-%m-%d\").values.item()\n",
    "    \n",
    "    if not first_date.endswith(\"-01-01\"):\n",
    "        first_year = int(first_date[:4])\n",
    "        data = data.sel(time=slice(str(first_year + 1), None))\n",
    "\n",
    "    precip = data[\"precip\"]\n",
    "    tmin = data[\"tmin\"]\n",
    "    tmax = data[\"tmax\"]\n",
    "    \n",
    "    return precip, tmin, tmax\n",
    "    \n",
    "def calc_spei_and_params(\n",
    "    precip, tmin, tmax, \n",
    "    agg_freq, start_date, end_date, \n",
    "    lat=None, dist=\"fisk\", method=\"ML\"):\n",
    "    \n",
    "    # --- Daily water budget for full period\n",
    "    wb = water_budget(pr=precip, tasmin=tmin, tasmax=tmax, method=\"HG85\", lat=lat)\n",
    "    wb.attrs[\"units\"] = \"kg m-2 s-1\"\n",
    "\n",
    "    # --- Fit params on the calibration period (monthly aggregation + rolling handled inside)\n",
    "    params = standardized_index_fit_params(\n",
    "        wb.sel(time=slice(start_date, end_date)),\n",
    "        freq=\"MS\",           # aggregate daily WB to monthly\n",
    "        window=agg_freq,     # e.g., 12 for SPEI-12\n",
    "        dist=dist,           # \"fisk\" = 3-parameter log-logistic\n",
    "        method=method,       # \"PWM\" (L-moments) is robust; \"ML\" for MLE\n",
    "    )\n",
    "\n",
    "    # --- Apply those params to the full record to get SPEI (ensures consistency)\n",
    "    spei = standardized_index(\n",
    "        wb,\n",
    "        freq=\"MS\",\n",
    "        window=agg_freq,\n",
    "        dist=dist,\n",
    "        method=method,\n",
    "        params=params,       # << use fixed calibration parameters\n",
    "        zero_inflated=False,    # for water balance, not zero-inflated\n",
    "        fitkwargs=None,         # or {}\n",
    "        cal_start=None,         # not needed when params are supplied\n",
    "        cal_end=None,   \n",
    "    )\n",
    "\n",
    "    # spei = standardized_precipitation_evapotranspiration_index(\n",
    "    #     wb=wb, freq=\"MS\", window=agg_freq, dist=dist, method=method, params=params\n",
    "    # )\n",
    "    \n",
    "    if np.nanmin(spei.values) < -3:\n",
    "        spei = spei.where(spei >= -3, np.nan).interpolate_na(\"time\")\n",
    "\n",
    "    return params, spei\n",
    "\n",
    "def noleap_to_gregorian_add_leap(ds: xr.Dataset, time_dim: str = \"time\") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Convert cftime.DatetimeNoLeap time coord to pandas DatetimeIndex (Gregorian)\n",
    "    and add Feb 29 for leap years by reindexing to a complete daily index and\n",
    "    linearly filling inserted dates.\n",
    "    \"\"\"\n",
    "    # 1) CFTimeIndex -> pandas DatetimeIndex (drops Feb 29 by definition)\n",
    "    cft = ds.indexes[time_dim]                 # xarray.coding.cftimeindex.CFTimeIndex\n",
    "    pd_idx = cft.to_datetimeindex()           # pandas.DatetimeIndex\n",
    "    ds = ds.assign_coords({time_dim: pd_idx})\n",
    "\n",
    "    # 2) Build full daily Gregorian index (includes Feb 29 when applicable)\n",
    "    full_idx = pd.date_range(pd_idx[0], pd_idx[-1], freq=\"D\")\n",
    "\n",
    "    # 3) Reindex to insert missing days (Feb 29 becomes NaN rows)\n",
    "    ds2 = ds.reindex({time_dim: full_idx})\n",
    "\n",
    "    # 4) Fill inserted NaNs by linear interpolation in time\n",
    "    #    (works for numeric variables; keeps non-numeric as-is)\n",
    "    num_vars = [v for v in ds2.data_vars if ds2[v].dtype.kind in \"fiu\"]\n",
    "    ds2[num_vars] = ds2[num_vars].interpolate_na(time_dim, method=\"linear\")\n",
    "\n",
    "    return ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f3e9c36-cc44-45d6-a6da-548b39513d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_lead_and_stack(ds_list, new_dim=\"member\", labels=None, ref=\"first\"):\n",
    "    \"\"\"\n",
    "    ds_list: list[xr.Dataset], each has a 'time' coord with daily values\n",
    "    new_dim: name of the new dimension to stack on\n",
    "    labels: optional labels for new_dim (len == len(ds_list))\n",
    "    ref: \"first\" (per-dataset first time) or a numpy/pandas datetime-like scalar\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i, ds in enumerate(ds_list):\n",
    "        ds = ds.sortby(\"time\")\n",
    "\n",
    "        if ref == \"first\":\n",
    "            t0 = ds[\"time\"].isel(time=0)\n",
    "        else:\n",
    "            # global reference (same for all ds)\n",
    "            t0 = xr.DataArray(ref)\n",
    "\n",
    "        lead = (ds[\"time\"] - t0).astype(\"timedelta64[D]\")  # daily deltas\n",
    "        ds2 = ds.assign_coords(lead_time=(\"time\", lead.data)).swap_dims({\"time\": \"lead_time\"})\n",
    "        ds2 = ds2.drop_vars(\"time\")  # optional: remove original coordinate\n",
    "\n",
    "        out.append(ds2)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = list(range(len(out)))\n",
    "\n",
    "    stacked = xr.concat(out, dim=xr.IndexVariable(new_dim, labels))\n",
    "    return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ab40e3-9115-443c-9b13-d9e37a447b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan_linear_2d(a):\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    ny, nx = a.shape\n",
    "    yy, xx = np.mgrid[0:ny, 0:nx]\n",
    "\n",
    "    mask = np.isfinite(a)\n",
    "    pts = np.column_stack((yy[mask], xx[mask]))   # (row, col) for valid points\n",
    "    vals = a[mask]\n",
    "\n",
    "    # Linear interpolation inside the convex hull of valid points\n",
    "    filled = a.copy()\n",
    "    filled[~mask] = griddata(pts, vals, (yy[~mask], xx[~mask]), method=\"linear\")\n",
    "\n",
    "    # Optional: fill any remaining NaNs (outside convex hull) with nearest\n",
    "    if np.any(~np.isfinite(filled)):\n",
    "        filled[~np.isfinite(filled)] = griddata(pts, vals, (yy[~np.isfinite(filled)], xx[~np.isfinite(filled)]),\n",
    "                                                method=\"nearest\")\n",
    "    return filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbfddbe9-8027-48f9-a518-877a6f4dfce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn_ERA5 = f'/glade/derecho/scratch/ksha/EPRI_data/ERA5_grid/ERA5_{year}.zarr'\n",
    "# ds_ERA5 = xr.open_zarr(fn_ERA5)\n",
    "# fn_CESM = f'/glade/derecho/scratch/ksha/EPRI_data/CESM2_SMYLE/SMYLE_{year-1}-11-01_daily_ensemble.zarr'\n",
    "# ds_CESM = xr.open_zarr(fn_CESM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc0587f-4640-4dc1-801b-7d4327ac40ca",
   "metadata": {},
   "source": [
    "### Preparing gridded yearly metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a25379fe-ce3e-4281-8107-5db321e0ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_loc = {\n",
    "    'Pituffik': (76.4, -68.575),\n",
    "    'Fairbanks': (64.75, -147.4),\n",
    "    'Guam': (13.475, 144.75),\n",
    "    'Yuma_PG': (33.125, -114.125),\n",
    "    'Fort_Bragg': (35.05, -79.115),\n",
    "}\n",
    "keys = list(dict_loc.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383a807b-7dbd-4cd3-8625-be98eb42b032",
   "metadata": {},
   "source": [
    "### Fort_Bragg: max total precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9e1699d-1078-44fa-9fae-866f16a5e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'Fort_Bragg'\n",
    "dir_stn = f'/glade/derecho/scratch/ksha/EPRI_data/METRICS/{key}/'\n",
    "base_dir = '/glade/derecho/scratch/ksha/EPRI_data/CESM2_SMYLE/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb15a704-48e6-42b8-b35a-44abb2d3b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_collection = []\n",
    "\n",
    "for year in range(1958, 2020):\n",
    "    \n",
    "    # get data and variable\n",
    "    fn_CESM = base_dir + f'/{key}/SMYLE_{key}_{year}.zarr'\n",
    "    ds_CESM = xr.open_zarr(fn_CESM)[[\"PRECT\"]]\n",
    "    ds_CESM['PRECT'] = ds_CESM['PRECT'] * 60*60*24 * 1000\n",
    "    ds_CESM = ds_CESM.sel(time=slice(f'{year+1}-01-01T00', f'{year+10}-12-31T00'))\n",
    "    \n",
    "    # ============ #\n",
    "    # PRECT max and weekly max\n",
    "    ds_TP_group = ds_CESM[[\"PRECT\"]].groupby(\"time.year\")\n",
    "    ds_TP_max  = ds_TP_group.max(dim=\"time\",  skipna=True)\n",
    "    ds_TP_3d = ds_TP_group.map(\n",
    "        lambda x: x.rolling(time=3, min_periods=3).mean().max(dim=\"time\", skipna=True)\n",
    "    )\n",
    "    ds_TP_5d = ds_TP_group.map(\n",
    "        lambda x: x.rolling(time=5, min_periods=5).mean().max(dim=\"time\", skipna=True)\n",
    "    )\n",
    "    \n",
    "    # rename and merge ds_mean, ds_min, ds_max, ds_30d_max\n",
    "    ds_TP_max = ds_TP_max.rename({v: f\"{v}_max\" for v in ds_TP_max.data_vars})\n",
    "    ds_TP_3d = ds_TP_3d.rename({v: f\"{v}_3d_max\"  for v in ds_TP_3d.data_vars})\n",
    "    ds_TP_5d = ds_TP_5d.rename({v: f\"{v}_5d_max\"  for v in ds_TP_5d.data_vars})\n",
    "    \n",
    "    # ============ #\n",
    "    ds_merge = xr.merge([ds_TP_max, ds_TP_3d, ds_TP_5d])\n",
    "    ds_merge = ds_merge.assign_coords({'year': np.arange(year, year+10) - year})\n",
    "    ds_collection.append(ds_merge)\n",
    "    \n",
    "ds_all = xr.concat(ds_collection, dim='init_time')\n",
    "ds_all = ds_all.assign_coords({'init_time': np.arange(1958+1, 2020+1)})\n",
    "ds_all = ds_all.chunk({'init_time': 62, 'year': 10, 'lat': 21, 'lon': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76871a3f-5f33-4603-9f0a-ef90f1ae28b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = dir_stn + 'CESM_TP_max.zarr'\n",
    "# ds_all.to_zarr(save_name, mode='w')\n",
    "print(save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfd4616-c82c-4467-b67a-ef1736a7045d",
   "metadata": {},
   "source": [
    "### Fort_Bragg: max detrend total precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0795617b-e1dc-417c-94fe-a23199902131",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = []\n",
    "\n",
    "for year in range(1958, 2020):\n",
    "    \n",
    "    # get data and variable\n",
    "    fn_CESM = base_dir + f'/{key}/SMYLE_{key}_{year}.zarr'\n",
    "    ds_CESM = xr.open_zarr(fn_CESM)[[\"PRECT\"]]\n",
    "    ds_CESM['PRECT'] = ds_CESM['PRECT'] * 60*60*24 * 1000\n",
    "    ds_CESM = ds_CESM.sel(time=slice(f'{year+1}-01-01T00', f'{year+10}-12-31T00'))\n",
    "    list_ds.append(ds_CESM )\n",
    "\n",
    "ds_all = time_to_lead_and_stack(list_ds, new_dim=\"init_time\")\n",
    "ds_all = ds_all.assign_coords({'init_time': np.arange(1959, 2021)})\n",
    "lead_year = (ds_all[\"lead_time\"] / np.timedelta64(365, \"D\")).astype(int)\n",
    "ds_all = ds_all.assign_coords(lead_year=(\"lead_time\", lead_year.data))\n",
    "\n",
    "# ========================== #\n",
    "# get detrend data\n",
    "ds_all_detrend = ds_all.copy()\n",
    "vars_ = list(ds_all.keys())\n",
    "for v in vars_:\n",
    "    ds_all_detrend[v] = detrend_linear(ds_all[v], dim=\"init_time\")\n",
    "ds_all_detrend = ds_all_detrend[vars_]\n",
    "\n",
    "ds_all_detrend = ds_all_detrend.rename({'lead_time': 'time'})\n",
    "ds_TP_group = ds_all_detrend.groupby(\"lead_year\")\n",
    "\n",
    "ds_TP_max  = ds_TP_group.max(dim=\"time\",  skipna=True)\n",
    "ds_TP_3d = ds_TP_group.map(\n",
    "    lambda x: x.rolling(time=3, min_periods=3).mean().max(dim=\"time\", skipna=True)\n",
    ")\n",
    "ds_TP_5d = ds_TP_group.map(\n",
    "    lambda x: x.rolling(time=5, min_periods=5).mean().max(dim=\"time\", skipna=True)\n",
    ")\n",
    "# rename and merge ds_mean, ds_min, ds_max, ds_30d_max\n",
    "ds_TP_max = ds_TP_max.rename({v: f\"{v}_max\" for v in ds_TP_max.data_vars})\n",
    "ds_TP_3d = ds_TP_3d.rename({v: f\"{v}_3d_max\"  for v in ds_TP_3d.data_vars})\n",
    "ds_TP_5d = ds_TP_5d.rename({v: f\"{v}_5d_max\"  for v in ds_TP_5d.data_vars})\n",
    "\n",
    "# ============ #\n",
    "ds_merge = xr.merge([ds_TP_max, ds_TP_3d, ds_TP_5d])\n",
    "ds_merge = ds_merge.rename({'lead_year': 'year'})\n",
    "ds_merge = ds_merge.chunk({'init_time': 62, 'year': 10, 'lat': 21, 'lon': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4909b47-e577-4af0-a749-d69d3b1b5f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = dir_stn + 'CESM_TP_detrend_max.zarr'\n",
    "# ds_merge.to_zarr(save_name, mode='w')\n",
    "print(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede16cf3-d1f7-450b-a3bf-a795261dd22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6d51e-45d4-4689-ab58-91819a96fba1",
   "metadata": {},
   "source": [
    "### Fort_Bragg: SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2670af8c-6abc-4f47-b08c-cd8b3dfb51ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_example = xr.open_zarr(base_dir+f'/{key}/SMYLE_{key}_2019.zarr')\n",
    "lat = ds_example['lat'].values\n",
    "lon = ds_example['lon'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52d225a9-1d71-49d2-a3f0-1c0565095b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nx = len(lat)\n",
    "# Ny = len(lon)\n",
    "\n",
    "# SPEI_09 = np.empty((10, 12*(2020-1959+1), Nx, Ny))\n",
    "# SPEI_09[...] = np.nan\n",
    "\n",
    "# SPEI_48 = np.empty((10, 12*(2020-1959+1), Nx, Ny))\n",
    "# SPEI_48[...] = np.nan\n",
    "\n",
    "# # 62 years x 12 month\n",
    "\n",
    "# for ind_lat in range(Nx):\n",
    "#     for ind_lon in range(Ny):\n",
    "\n",
    "#         t0 = time.process_time()\n",
    "#         print(f'({ind_lat}, {ind_lon})')\n",
    "        \n",
    "#         for lead_year in range(0, 10):\n",
    "              \n",
    "#             start_date = f\"{1959+lead_year}-01-01T00\"\n",
    "#             end_date = f\"{2020+lead_year}-12-31T23\"\n",
    "            \n",
    "#             ds_collection = []\n",
    "            \n",
    "#             for year_init in range(1958, 2020, 1):\n",
    "                \n",
    "#                 year_start = year_init + 1 + lead_year\n",
    "#                 time_start = f'{year_start}-01-01T00'\n",
    "#                 time_end = f'{year_start}-12-31T00'\n",
    "                \n",
    "#                 fn_CESM = base_dir + f'/{key}/SMYLE_{key}_{year_init}.zarr'\n",
    "#                 ds_CESM = xr.open_zarr(fn_CESM)[['TREFHTMN', 'TREFHTMX', 'PRECT']].isel(lon=ind_lon, lat=ind_lat)\n",
    "                \n",
    "#                 ds_CESM = ds_CESM.sel(time=slice(time_start, time_end))\n",
    "#                 ds_collection.append(ds_CESM)\n",
    "            \n",
    "#             ds_all = xr.concat(ds_collection, dim='time')\n",
    "#             ds_all = ds_all.load()\n",
    "            \n",
    "#             cft = ds_all.indexes['time']\n",
    "#             pd_idx = cft.to_datetimeindex()\n",
    "#             ds_all = ds_all.assign_coords({'time': pd_idx})\n",
    "            \n",
    "#             lat_ref = ds_all['lat'].values\n",
    "#             lat_mid = lat_ref # lat_ref[ind_lat]\n",
    "#             time_vals = ds_all['time']\n",
    "            \n",
    "#             tmin = ds_all['TREFHTMN'].values\n",
    "#             tmax = ds_all['TREFHTMX'].values\n",
    "#             precip = ds_all['PRECT'].values\n",
    "            \n",
    "#             ds = xr.Dataset(\n",
    "#                 {\n",
    "#                     \"precip\": ((\"time\",), precip*1e3, {\"units\": \"kg m-2 s-1\"}),\n",
    "#                     \"tmin\":   ((\"time\",), tmin-273.15, {\"units\": \"degC\"}),\n",
    "#                     \"tmax\":   ((\"time\",), tmax-273.15, {\"units\": \"degC\"}),\n",
    "#                 },\n",
    "#                 coords={\"time\": time_vals, \"lat\": lat_mid}\n",
    "#             )\n",
    "            \n",
    "#             for v in (\"precip\", \"tmin\", \"tmax\"):\n",
    "                \n",
    "#                 ds[v] = ds[v].assign_coords(lat=lat_mid)\n",
    "                \n",
    "#                 ds[v][\"lat\"].attrs = {\n",
    "#                     \"standard_name\": \"latitude\",\n",
    "#                     \"units\": \"degrees_north\", \"axis\": \"Y\"\n",
    "#                 }\n",
    "            \n",
    "#             precip, tmin, tmax = process_vars(ds)\n",
    "\n",
    "#             # ---------------------------------- #\n",
    "#             # 24 month lagged SPEI\n",
    "#             params, spei = calc_spei_and_params(\n",
    "#                 precip, tmin, tmax, \n",
    "#                 agg_freq=9, \n",
    "#                 start_date=start_date,\n",
    "#                 end_date=end_date,\n",
    "#                 lat=precip[\"lat\"],\n",
    "#                 dist=\"fisk\", method=\"ML\"\n",
    "#             )\n",
    "            \n",
    "#             SPEI_09[lead_year, :, ind_lat, ind_lon] = spei.values\n",
    "\n",
    "#             # ---------------------------------- #\n",
    "#             # 48 month lagged SPEI\n",
    "#             params, spei = calc_spei_and_params(\n",
    "#                 precip, tmin, tmax, \n",
    "#                 agg_freq=48, \n",
    "#                 start_date=start_date,\n",
    "#                 end_date=end_date,\n",
    "#                 lat=precip[\"lat\"],\n",
    "#                 dist=\"fisk\", method=\"ML\"\n",
    "#             )\n",
    "            \n",
    "#             SPEI_48[lead_year, :, ind_lat, ind_lon] = spei.values\n",
    "            \n",
    "#         t1 = time.process_time()\n",
    "#         print(f\"time: {t1 - t0:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "952fbc02-89ee-4ca5-ae27-bb8b3130c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx = len(lat)\n",
    "Ny = len(lon)\n",
    "\n",
    "SPEI_09 = np.empty((10, 12*(2020-1959+1), Nx, Ny))\n",
    "SPEI_09[...] = np.nan\n",
    "\n",
    "SPEI_48 = np.empty((10, 12*(2020-1959+1), Nx, Ny))\n",
    "SPEI_48[...] = np.nan\n",
    "\n",
    "for ind_lat in range(21):\n",
    "    fn = dir_stn + f'temp_np/SPEI_{ind_lat}.npy'\n",
    "    data_ = np.load(fn)\n",
    "    SPEI_09[:, :, ind_lat, :] = data_[..., 0]\n",
    "    SPEI_48[:, :, ind_lat, :] = data_[..., 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7eda4e2-fba9-4b5d-9de5-6b9c1c9d5ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/glade/derecho/scratch/ksha/EPRI_data/METRICS/Fort_Bragg/temp_np/SPEI_20.npy'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b5a4c3b-ac76-4966-9c35-4c940e354d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================= #\n",
    "# fill nans using linear interp\n",
    "\n",
    "for lead_year in range(10):\n",
    "    for i_time in range(12*(2020-1959+1)):\n",
    "        \n",
    "        spei_temp_09 = SPEI_09[lead_year, i_time, :, :]\n",
    "        flag_nan_09 = np.sum(np.isnan(spei_temp_09))\n",
    "        \n",
    "        spei_temp_48 = SPEI_48[lead_year, i_time, :, :]\n",
    "        flag_nan_48 = np.sum(np.isnan(spei_temp_48))\n",
    "\n",
    "        # need to have at least 4 values in the domain\n",
    "        if (flag_nan_09 < (Nx*Ny-4)) and (flag_nan_09 > 0):\n",
    "            SPEI_09[lead_year, i_time, :, :] = fill_nan_linear_2d(spei_temp_09)\n",
    "            \n",
    "        if (flag_nan_48 < (Nx*Ny-4)) and (flag_nan_48 > 0):\n",
    "            SPEI_48[lead_year, i_time, :, :] = fill_nan_linear_2d(spei_temp_48)\n",
    "\n",
    "# ======================================= #\n",
    "# numpy to ds\n",
    "\n",
    "n_lead = 10\n",
    "n_init = 62\n",
    "m_per_year = 12\n",
    "\n",
    "# (10, 744, Nx, Ny) -> (10, 62, 12, Nx, Ny)\n",
    "tmp_09 = SPEI_09.reshape(n_lead, n_init, m_per_year, *SPEI_09.shape[2:])\n",
    "tmp_48 = SPEI_48.reshape(n_lead, n_init, m_per_year, *SPEI_48.shape[2:])\n",
    "\n",
    "# (10, 62, 12, Nx, Ny) -> (62, 10, 12, Nx, Ny)\n",
    "tmp_09 = tmp_09.transpose(1, 0, 2, 3, 4)\n",
    "tmp_48 = tmp_48.transpose(1, 0, 2, 3, 4)\n",
    "\n",
    "# (62, 10, 12, Nx, Ny) -> (62, 120, Nx, Ny)\n",
    "SPEI_init_09 = tmp_09.reshape(n_init, n_lead * m_per_year, *SPEI_09.shape[2:])\n",
    "SPEI_init_48 = tmp_48.reshape(n_init, n_lead * m_per_year, *SPEI_48.shape[2:])\n",
    "\n",
    "ds_SPEI = xr.Dataset(\n",
    "    data_vars={\n",
    "        \"SPEI_09\": ((\"init_time\", \"lead_time_month\", \"lat\", \"lon\"), SPEI_init_09),\n",
    "        \"SPEI_48\": ((\"init_time\", \"lead_time_month\", \"lat\", \"lon\"), SPEI_init_48)\n",
    "    },\n",
    "    coords={\n",
    "        \"init_time\": np.arange(1958, 2020),\n",
    "        \"lead_time_month\": np.arange(120),\n",
    "        \"lat\": lat,\n",
    "        \"lon\": lon,\n",
    "    },\n",
    ")\n",
    "\n",
    "# optional metadata\n",
    "ds_SPEI[\"SPEI_09\"].attrs[\"long_name\"] = \"SPEI with 09 month lag\"\n",
    "ds_SPEI[\"SPEI_48\"].attrs[\"long_name\"] = \"SPEI with 48 month lag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27600e05-a0df-4628-a226-7e54d3328784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS/Fort_Bragg/CESM_SPEI.zarr\n"
     ]
    }
   ],
   "source": [
    "save_name = dir_stn + 'CESM_SPEI.zarr'\n",
    "# ds_SPEI.to_zarr(save_name)\n",
    "print(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "925949cc-26c6-4291-957c-e227e884cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(SPEI_09[lead_year, :, 10, 11])\n",
    "# plt.plot(SPEI_48[lead_year, :, 10, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f7820-82fb-41e5-9b01-36c0d1436e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
