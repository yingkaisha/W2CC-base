{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe482de-6ad1-438b-9df5-306064461747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a57168-76cd-431e-aa6a-f280212e959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zarr\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53879a61-42cf-4a43-8c28-46cef84c2a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887732a0-ad4b-48ec-8d45-4e43d95e225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd54f27d-e5a1-41c6-ad17-8dfb62cafc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detrend_linear(da, dim=\"time\"):\n",
    "#     \"\"\"\n",
    "#     Remove a best-fit linear trend along `dim` for each grid point.\n",
    "#     Uses an index-based time axis (0..N-1) to avoid datetime scaling issues.\n",
    "#     \"\"\"\n",
    "#     t = xr.DataArray(np.arange(da.sizes[dim]), dims=dim, coords={dim: da[dim]})\n",
    "\n",
    "#     valid = np.isfinite(da)\n",
    "#     t_valid = t.where(valid)\n",
    "#     da_valid = da.where(valid)\n",
    "\n",
    "#     t_mean = t_valid.mean(dim, skipna=True)\n",
    "#     y_mean = da_valid.mean(dim, skipna=True)\n",
    "\n",
    "#     cov = ((t_valid - t_mean) * (da_valid - y_mean)).mean(dim, skipna=True)\n",
    "#     var = ((t_valid - t_mean) ** 2).mean(dim, skipna=True)\n",
    "\n",
    "#     slope = cov / var\n",
    "#     intercept = y_mean - slope * t_mean\n",
    "\n",
    "#     trend = slope * t + intercept\n",
    "#     return da - trend\n",
    "\n",
    "def detrend_linear_doy(da: xr.DataArray, dim: str = \"time\", keep_mean: bool = True) -> xr.DataArray:\n",
    "    t = da[dim]\n",
    "    doy = t.dt.dayofyear\n",
    "    x = t.dt.year.astype(\"float64\")  # predictor\n",
    "\n",
    "    # If you have missing values, this keeps x/y stats consistent\n",
    "    mask = da.notnull()\n",
    "\n",
    "    # Groupwise means (per DOY)\n",
    "    x_mean = x.where(mask).groupby(doy).mean(dim, skipna=True)       # dims: dayofyear\n",
    "    y_mean = da.groupby(doy).mean(dim, skipna=True)                  # dims: dayofyear + (other dims)\n",
    "\n",
    "    # Broadcast group means back onto the time axis\n",
    "    x_anom = x - x_mean.sel(dayofyear=doy)                           # dims: time\n",
    "    y_anom = da - y_mean.sel(dayofyear=doy)                          # dims: time + (other dims)\n",
    "\n",
    "    # Least-squares slope per DOY: slope = cov(x,y)/var(x)\n",
    "    numer = (x_anom * y_anom).where(mask).groupby(doy).mean(dim, skipna=True)\n",
    "    denom = (x_anom ** 2).where(mask).groupby(doy).mean(dim, skipna=True)\n",
    "    slope = numer / denom                                            # dims: dayofyear + (other dims)\n",
    "\n",
    "    if keep_mean:\n",
    "        # Remove slope only; preserves the DOY mean exactly\n",
    "        return da - slope.sel(dayofyear=doy) * x_anom\n",
    "    else:\n",
    "        # Remove full fitted line (slope + intercept)\n",
    "        intercept = y_mean - slope * x_mean\n",
    "        fit = slope.sel(dayofyear=doy) * x + intercept.sel(dayofyear=doy)\n",
    "        return da - fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246b0bb-697f-4a2d-9307-8b1a111a44c4",
   "metadata": {},
   "source": [
    "## ERA5 hourly to daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd21a0a-8966-42ea-87f6-f966e4cd38b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# station_names = ['Pituffik', 'Fairbanks', 'Guam', 'Yuma_PG' ,'Fort_Bragg'] # \n",
    "\n",
    "# for stn in station_names:\n",
    "#     base_dir = f'/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/{stn}/'\n",
    "    \n",
    "#     # ========================== #\n",
    "#     # get data\n",
    "#     list_ds = []\n",
    "#     for year in range(1958, 2025):\n",
    "#         fn = f'/glade/campaign/ral/hap/ksha/EPRI_data/ERA5_daily/{stn}_{year}.zarr'\n",
    "#         ds = xr.open_zarr(fn)\n",
    "#         list_ds.append(ds)\n",
    "        \n",
    "#     ds_all = xr.concat(list_ds, dim='time')\n",
    "#     ds_all['PRECT'] = ds_all['PRECT'] * 1000 # mm per day\n",
    "    \n",
    "#     # ========================== #\n",
    "#     # get anomaly\n",
    "#     ds_all_anom = ds_all.copy()\n",
    "#     vars_ = list(ds_all.keys())\n",
    "#     for v in vars_:\n",
    "#         clim = ds_all[v].groupby(\"time.dayofyear\").mean(\"time\", keep_attrs=True)\n",
    "#         ds_all_anom[v] = ds_all[v].groupby(\"time.dayofyear\") - clim\n",
    "#     ds_all_anom = ds_all_anom[vars_]\n",
    "    \n",
    "#     # ========================== #\n",
    "#     # get detrend data\n",
    "#     ds_all_detrend = ds_all.copy()\n",
    "#     vars_ = list(ds_all.keys())\n",
    "#     for v in vars_:\n",
    "#         ds_all_detrend[v] = detrend_linear_doy(ds_all[v], dim=\"time\", keep_mean=False)\n",
    "#         # detrend_linear(ds_all[v], dim=\"time\")\n",
    "#     ds_all_detrend = ds_all_detrend[vars_]\n",
    "    \n",
    "#     # ======================= #\n",
    "#     # metrics\n",
    "#     ds_group = ds_all.groupby(\"time.year\")\n",
    "#     ds_max  = ds_group.max(dim=\"time\",  skipna=True)\n",
    "#     ds_min  = ds_group.min(dim=\"time\",  skipna=True)\n",
    "#     ds_mean  = ds_group.mean(dim=\"time\",  skipna=True)\n",
    "#     ds_30d = ds_group.map(\n",
    "#         lambda x: x.rolling(time=30, min_periods=30).mean().max(dim=\"time\", skipna=True)\n",
    "#     )\n",
    "#     # ds_min = ds_min.rename({'TREFHTMN': 'TREFHTMN_min'})[['TREFHTMN_min',]]\n",
    "#     # ds_max = ds_max.rename({'PRECT': 'PRECT_max', 'TREFHTMX': 'TREFHTMX_max'})[['PRECT_max', 'TREFHTMX_max']]\n",
    "#     # ds_30d = ds_30d.rename({'TREFHTMX': 'TREFHTMX_30d', 'PRECT': 'PRECT_30d'})[['TREFHTMX_30d', 'PRECT_30d']]\n",
    "#     # ds_mean = ds_mean.rename({'PRECT': 'PRECT_mean', 'TREFHT': 'TREFHT_mean'})[['PRECT_mean', 'TREFHT_mean']]\n",
    "#     ds_min = ds_min.rename({'TREFHTMN': 'TREFHTMN_min', 'TREFHT': 'TREFHT_min'})[['TREFHTMN_min', 'TREFHT_min']]\n",
    "#     ds_max = ds_max.rename({'PRECT': 'PRECT_max', 'TREFHTMX': 'TREFHTMX_max', 'TREFHT': 'TREFHT_max'})[['PRECT_max', 'TREFHTMX_max', 'TREFHT_max']]\n",
    "#     ds_30d = ds_30d.rename({'TREFHT': 'TREFHT_30d', 'PRECT': 'PRECT_30d'})[['TREFHT_30d', 'PRECT_30d']]\n",
    "#     ds_mean = ds_mean.rename({'PRECT': 'PRECT_mean', 'TREFHT': 'TREFHT_mean'})[['PRECT_mean', 'TREFHT_mean']]\n",
    "#     ds_metrics = xr.merge([ds_min, ds_max, ds_30d, ds_mean])\n",
    "#     ds_metrics = ds_metrics.rename({v: f\"{v}_default\" for v in ds_metrics.data_vars})\n",
    "\n",
    "#     # ========================== #\n",
    "#     # anomaly metrics\n",
    "#     ds_group = ds_all_anom.groupby(\"time.year\")\n",
    "#     ds_max  = ds_group.max(dim=\"time\",  skipna=True)\n",
    "#     ds_min  = ds_group.min(dim=\"time\",  skipna=True)\n",
    "#     ds_mean  = ds_group.mean(dim=\"time\",  skipna=True)\n",
    "#     ds_30d = ds_group.map(\n",
    "#         lambda x: x.rolling(time=30, min_periods=30).mean().max(dim=\"time\", skipna=True)\n",
    "#     )\n",
    "#     ds_min = ds_min.rename({'TREFHTMN': 'TREFHTMN_min', 'TREFHT': 'TREFHT_min'})[['TREFHTMN_min', 'TREFHT_min']]\n",
    "#     ds_max = ds_max.rename({'PRECT': 'PRECT_max', 'TREFHTMX': 'TREFHTMX_max', 'TREFHT': 'TREFHT_max'})[['PRECT_max', 'TREFHTMX_max', 'TREFHT_max']]\n",
    "#     ds_30d = ds_30d.rename({'TREFHT': 'TREFHT_30d', 'PRECT': 'PRECT_30d'})[['TREFHT_30d', 'PRECT_30d']]\n",
    "#     ds_mean = ds_mean.rename({'PRECT': 'PRECT_mean', 'TREFHT': 'TREFHT_mean'})[['PRECT_mean', 'TREFHT_mean']]\n",
    "#     ds_metrics_anom = xr.merge([ds_min, ds_max, ds_30d, ds_mean])\n",
    "#     ds_metrics_anom = ds_metrics_anom.rename({v: f\"{v}_anom\" for v in ds_metrics_anom.data_vars})\n",
    "\n",
    "#     # ========================== #\n",
    "#     # detrended metrics\n",
    "#     ds_group = ds_all_detrend.groupby(\"time.year\")\n",
    "#     ds_max  = ds_group.max(dim=\"time\",  skipna=True)\n",
    "#     ds_min  = ds_group.min(dim=\"time\",  skipna=True)\n",
    "#     ds_mean  = ds_group.mean(dim=\"time\",  skipna=True)\n",
    "#     ds_30d = ds_group.map(\n",
    "#         lambda x: x.rolling(time=30, min_periods=30).mean().max(dim=\"time\", skipna=True)\n",
    "#     )\n",
    "#     ds_min = ds_min.rename({'TREFHTMN': 'TREFHTMN_min', 'TREFHT': 'TREFHT_min'})[['TREFHTMN_min', 'TREFHT_min']]\n",
    "#     ds_max = ds_max.rename({'PRECT': 'PRECT_max', 'TREFHTMX': 'TREFHTMX_max', 'TREFHT': 'TREFHT_max'})[['PRECT_max', 'TREFHTMX_max', 'TREFHT_max']]\n",
    "#     ds_30d = ds_30d.rename({'TREFHT': 'TREFHT_30d', 'PRECT': 'PRECT_30d'})[['TREFHT_30d', 'PRECT_30d']]\n",
    "#     ds_mean = ds_mean.rename({'PRECT': 'PRECT_mean', 'TREFHT': 'TREFHT_mean'})[['PRECT_mean', 'TREFHT_mean']]\n",
    "#     ds_metrics_detrend = xr.merge([ds_min, ds_max, ds_30d, ds_mean])\n",
    "#     ds_metrics_detrend = ds_metrics_detrend.rename({v: f\"{v}_detrend\" for v in ds_metrics_detrend.data_vars})\n",
    "\n",
    "#     # ========================== #\n",
    "#     # save\n",
    "#     ds_final = xr.merge([ds_metrics, ds_metrics_anom, ds_metrics_detrend])\n",
    "#     save_name = base_dir + 'metrics.zarr'\n",
    "#     ds_final.to_zarr(save_name, mode='w')\n",
    "#     print(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9656b031-a232-445e-918c-4363abbee5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Pituffik/metrics.zarr\n",
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Fairbanks/metrics.zarr\n",
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Guam/metrics.zarr\n",
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Yuma_PG/metrics.zarr\n",
      "/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Fort_Bragg/metrics.zarr\n"
     ]
    }
   ],
   "source": [
    "station_names = ['Pituffik', 'Fairbanks', 'Guam', 'Yuma_PG' ,'Fort_Bragg']\n",
    "\n",
    "def annual_metrics(ds_in, suffix):\n",
    "    \"\"\"\n",
    "    Compute yearly min/max/mean and 30-day rolling-mean max, then rename + suffix.\n",
    "    Minimal change: same variables as your original.\n",
    "    \"\"\"\n",
    "    g = ds_in.groupby(\"time.year\")\n",
    "    ds_max  = g.max(\"time\", skipna=True)\n",
    "    ds_min  = g.min(\"time\", skipna=True)\n",
    "    ds_mean = g.mean(\"time\", skipna=True)\n",
    "    ds_30d = (\n",
    "        ds_in.rolling(time=30, min_periods=30).mean()\n",
    "            .groupby(\"time.year\").max(\"time\", skipna=True)\n",
    "    )\n",
    "\n",
    "    ds_min = ds_min.rename({'TREFHTMN': 'TREFHTMN_min', 'TREFHT': 'TREFHT_min'})[['TREFHTMN_min', 'TREFHT_min']]\n",
    "    ds_max = ds_max.rename({'PRECT': 'PRECT_max', 'TREFHTMX': 'TREFHTMX_max', 'TREFHT': 'TREFHT_max'})[['PRECT_max', 'TREFHTMX_max', 'TREFHT_max']]\n",
    "    ds_30d = ds_30d.rename({'TREFHT': 'TREFHT_30d', 'PRECT': 'PRECT_30d'})[['TREFHT_30d', 'PRECT_30d']]\n",
    "    ds_mean = ds_mean.rename({'PRECT': 'PRECT_mean', 'TREFHT': 'TREFHT_mean'})[['PRECT_mean', 'TREFHT_mean']]\n",
    "\n",
    "    ds_out = xr.merge([ds_min, ds_max, ds_30d, ds_mean])\n",
    "    return ds_out.rename({v: f\"{v}_{suffix}\" for v in ds_out.data_vars})\n",
    "\n",
    "for stn in station_names:\n",
    "    base_dir = f'/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/{stn}/'\n",
    "    \n",
    "    # get data\n",
    "    list_ds = []\n",
    "    for year in range(1958, 2025):\n",
    "        fn = f'/glade/campaign/ral/hap/ksha/EPRI_data/ERA5_daily/{stn}_{year}.zarr'\n",
    "        ds = xr.open_zarr(fn)\n",
    "        list_ds.append(ds)\n",
    "        \n",
    "    ds_all = xr.concat(list_ds, dim='time')\n",
    "    ds_all = ds_all[['PRECT', 'TREFHT', 'TREFHTMX', 'TREFHTMN']]\n",
    "\n",
    "    ds_all['PRECT'] = ds_all['PRECT'] * 1000  # mm per day\n",
    "    ds_all = ds_all.chunk({\"time\": -1})\n",
    "\n",
    "    # ======================= #\n",
    "    # metrics (compute via helper; avoids repeating logic)\n",
    "    ds_metrics_default = annual_metrics(ds_all, \"default\")\n",
    "\n",
    "    # ========================== #\n",
    "    # save\n",
    "    ds_final = ds_metrics_default \n",
    "    #xr.merge([ds_metrics_default, ds_metrics_anom, ds_metrics_detrend])\n",
    "    \n",
    "    save_name = base_dir + 'metrics.zarr'\n",
    "    ds_final.to_zarr(save_name, mode='w')\n",
    "    print(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "506748ae-13e0-4c7c-bac4-0fe0c3deb133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/glade/derecho/scratch/ksha/EPRI_data/METRICS_STN/Fort_Bragg/metrics.zarr'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b801c39-c11f-45d1-bc27-9c4572b29de0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
